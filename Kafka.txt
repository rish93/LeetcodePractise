**Intro:
*Apache Kafka is a popular event streaming platform
used for real time and batch processing.
Kafka architecture allows for scalable streaming data. I
Create multiple brokers and cluster of nodes to handle messages and store their replicas.
Then, monitor the settings and logs for those brokers.
Finally, see how topic partitions and replicas provide redundancy and maintain high availability.


*Kafka is distirbuted platform,which operate together ina cluster
 there are several nodes that work in cluster.

*Replicas : data stored in multiple brokers, in case of broker failure message can still be recorverd


**Streaming  type:

Batch vs Stream
-> Data is bounded(how mych data coming in)              ->  Data is not bounded(unknown data coming in)
-> Lag between data ingestion and processing           ->  data processed in real time
-> Order of data is not critical (eg loan issued in mothend report)       -> Correct ordering is critical
-> All data to be wwoprked on is known up front   ->  Processsing app does not know future data
-> runs until job is complete                    -> Always on, listening for new data

---> middleground Micro batch proicessing
> compromise between batch and stream processing
> data processed frequently in small batches
> outputs can be avaialble in near real time (customer experience is nearly met)
any delay is in tolerable limits


**Stream event with Kafka:

Streaming use case (fraud detection)
1) bank want to process al transactionin real time to check
for fraud (either bank t bank trasnation of debit card to bank)
2) If transaction is lgitimate, simply commit to relational database (later it can be accessd by account holder
or maybe by analysis team by bank in monthly or quarterly report)
3)  for any faraudulant transaction owner of account need to be notifi9ed
by flagginf the fraud and norifying by phone or mail quickly, because transaction cannot
be commited until user approves of it.


Component kafka:
-Topic> channel of communication is topic
-Producse consumer are decouppled
-Rate of production and consumption of message can vary significantly
-as kafka reliably store and buffer the data in topic
- topics can also be considered as means of
managing messages in kafka, single kafka cluster can have multiple topics
- each topic is seperate line of communication

fact: With Kafka's new KRaft mode, which entirely removes ZooKeeper from Kafka's architecture,
 a Kafka cluster can handle millions of topics/partitions.
 See https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/ for details
fact: kafka doesnt have bounded buffr poblem
https://www.cs.umb.edu/~eoneil/cs444_f06/class13.html


- topics can b considered as a link with multiple producer and consumers
- events are durably stored in a topic, even after they have been processed by consumers
i.e many to many form of coomunication

fact: Can we channel different kind of message through same kafka topic?
Kafka topics don't inheriently have "types of data". It's all bytes, so yes you can
serialize completely separate objects into the same topic,
but consumers must then add logic to know what are all
possible types will get added into the topic.
That being said, Structured Streaming is built on the idea of having
structured data with a schema, so it likely will not work if you had completely
different types in the same topic without at least performing
 a filter first based on some inner attribute that is always present among all types.

*Topic can be  partitioned to improve scalability
* Kafka topic can be replicated for fault tolerance
it can be replicate do ndifferent nodes of a cluster, if one node fail from cluster other node
can be refered with sam etopic having replica to consume message

What topics are not?
they are not message queue, message published to topic remain after they are consumed.
hence it can be
considered as unbounded buffer

Unlike message queue where messages are removed after consumed,
Multiple consumer may process the same data added to a topic


Kafka works in distributed environment, as there are multiple node that work
as cluster.
Each node is referred as a server or broker.

Coordination between broker is nmanaged by service named zookeeper


**Kafka Topic:

Kafka Broker: As kafka  is distributed platform,
as nodes working together as cluster
-Each host in system can be served as kafka broker
-Broker store partitions and their replicas
-Broker process request to read from ad  write to partitions

Kafka topics store message as durably as any production databases

Consumer may choose to process message since topic was created.
Message can b set to expire.

fact: Why message not deleted if retention policy exceeded or storage capacity reaches threshold
because of segment size and message in it click: https://dalelane.co.uk/blog/?p=3993

--Partition:
Topics maybe split into subsets called partitions
partition may be distributed across different brokers in kafka cluster.
Multiple consumers on same topic may process
event on diffeent partition
-Parttion allow topics to scale


--Distributing events oin parttion
>message published to a topic may include a key and value
>the asssginment to a partitoon is absed on operastion on the key
>developers may define their own partiioner

---Replicas

>Broker may occasionally fail, and parttion they manage can become unreachable
> Topics can be replicated so that their partitons have copies
distributed across the cluster

in multinode env kafka ewill ensure Data and replicaas are not placed in same broker
>Each partiton will have a lead replica and zero or more follower replicas
> writes are performed to leader and follower must try to keep the sync
> a follower which lags too far behind will no longer be considered "in sync"
read request is not performed until it comes back to sync




When a subscriber is running - Does it specify its group id so that it can be part of a cluster of consumers of the same topic or several topics that this group of consumers is interested in?

Yes, consumers join (or create if they're alone) a consumer group to share load.
No two consumers in the same group will ever receive the same message.



 Are the partitions created by the broker, therefore not a concern for the consumers?

They're not, but you can see from 3 that it's totally useless to have more consumers than existing partitions,
so it's your maximum parallelism level for consuming.

Since this is a queue with an offset for each partition,
is it responsibility of the consumer to specify which messages it wants to read? Does it need to save its state?

Yes, consumers save an offset per topic per partition. This is totally handled by Kafka, no worries about it.

Q. Apache Kafka and Amazon SQS are both used for message streaming but are not the same.

A. Apache Kafka follows the publish subscriber model, where the producer sends an event/message
 to a topic, and one or more consumers are subscribed to that topic to get the event/message.
  In the topic, you find partitions for parallel streaming. There is a consumer group concept once.
   When a message is read from a partition of topics it will be committed to identify it already read by
    that consumer group to avoid inconsistency in reading in concurrent programming. However, other consumer
     groups can still read that message from the partition.

Where Amazon SQS follows Queue and the queue can be created in any region of Amazon SQS.
You can push messages to Queue and only one consumer can subscribe to each Queue and pull
 messages from the Queue. **That's why SQS is pull-based streaming**. SQS Queues are of two types: FIFO and Standard.

There is another concept in AWS which is Amazon SNS, which is published subscriber-based like Kafka,
 but there is not any message retention policy in SNS. It's for instant messaging like email, SMS, etc.
 It can only push messages to subscribers when the subscribers are available. Otherwise, the message will be lost.
 However, SQS with SNS can overcome this drawback. Amazon SNS with SQS is called the fanout pattern.
  In this pattern, a message published to an SNS topic is distributed to multiple SQS queues
  in parallel and the SQS queue assures persistence, because SQS has a retention policy.
  It can persist message for up to 14 days(default 4 days). Amazon SQS with SNS can achieve high
  throughput parallel streaming and can replace Apache Kafka.


Messages sent to an Amazon SQS queue wait until a consumer requests a message. When the message is retrieved,
 it is made 'invisible' on the queue so no other consumer will receive it. When a consumer has finished
 processing the message, it deletes the message from the queue.

Therefore, if you want multiple consumers to receive the same message, then Amazon SQS is not the correct
service to use.

Since you want a publish/subscribe model, you should be using Amazon Simple Notification Service (Amazon SNS).
 Messages are published to a 'Topic' and multiple subscribers can receive messages sent to that topic.
 Subscribers can use Amazon SNS subscription filter policies - Amazon Simple Notification Service to
 limit which messages they receive.

Note that messages sent to an Amazon SNS topic are immediately sent to subscribers. If you do not
wish to receive a message in real-time, it is possible to subscribe an Amazon SQS queue to an Amazon
SNS topic. This way, the messages will be queued for later retrieval. The queue would work
independently to other subscribers on the topic.

The group ID is very important to how different consumers "load balance" partitions.
 For example, if you have a topic with 10 partitions then two consumers with the same groupId will read from 5 partitions each.

If you have two consumers with different group ids, both consumers will read from 10 partitions.

In this sense, the groupId is how you define a "consumer group" or group of consumers reading from a given topic/partitions.

If a topic has 3 partitions and you have 2 consumers operating within the same consumer group, one of the consumers will read from 2 partitions and the other will read from 1.

If a topic has 4 partitions and you have 2 consumers operating within the same group then both consumers will read from 2 partitions.

If a topic has 1 partition and you have 2 consumers then 1 consumer reads from 1 partition and the other just sits there...



What happens when a message is deleted from the queue? - For example: The retention was for 3 hours, then the time passes, how is the offset being handled on both sides?

If a consumer ever request an offset not available for a partition on the brokers (for example, due to deletion), it enters an error mode, and ultimately reset itself for this partition to either the most recent or the oldest message available (depending on the auto.offset.reset configuration value), and continue working.



Kafka is pull baed model:

Scalability was the major driving factor when we design such systems (pull vs push). Kafka is very scalable. One of the key benefits of Kafka is that it is very easy to add large number of consumers without affecting performance and without down time.

Kafka can handle events at 100k+ per second rate coming from producers. Because Kafka consumers pull data from the topic, different consumers can consume the messages at different pace. Kafka also supports different consumption models. You can have one consumer processing the messages at real-time and another consumer processing the messages in batch mode.

The other reason could be that Kafka was designed not only for single consumers like Hadoop. Different consumers can have diverse needs and capabilities.

Pull-based systems have some deficiencies like resources wasting due to polling regularly. Kafka supports a 'long polling' waiting mode until real data comes through to alleviate this drawback.